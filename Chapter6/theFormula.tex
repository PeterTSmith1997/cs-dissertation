\section{The Formula} \label{The Formula}

The formula is broken down into five distinct categories of information. These five variables were chosen as they each look at a different aspects of an attack. It is believed that a malicious IP instance can be detected and identified by looking at the values of these attributes. The code for the formula can be seen in appendix \ref{code}.

\subsection{Bots}

It is well known that bots are used by numerous online companies in order to index and categorise various web-pages. Although some of these instances of traffic can at first sight appear to be malicious, due to their architecture, they are in fact a legitimate form of web traffic. For this reason a process is required within the formula to disclude known bots from any further processing within the formula and disregard them at the identification process. This process should preferably be carried out at an early stage within the program. For this reason it has been decided to set this risk factor to zero for known bots, as they pose no risk to the website.

The data for bots is gathered from other websites and deposited in a database where an admin can sort them. The formula will access this database to search for known bots. According to \citeauthor{Bots} 17.5\% of web traffic was determined to be legitimate bot traffic, for example: google crawlers. These bots are vital for the maintenance and navigation of the internet, hence the importance of the software to disclude these IP addresses from potential barring. It is suggested that 20.4\% of all web traffic in 2018 was considered to be malicious (\cite{Bots}), most of which was automated in a bot like format.

Due to the fact that fake google bots or fake search engine bots exist online (\cite{algiryage2018distinguishing}), this first check could be altered in the future to look at whether a bot instance is legitimate. The decision was made not to implement this as part of the formula during this research, we shall discuss the reason for this decision during the final conclusions of the research.


\subsection{Number of Requests}
The literature would indicate that the more requests a website receives from a single IP, the greater the likelihood that the source IP is malicious. Thus, after consideration it was concluded that an IP occurrence may appear in the log files a large number of times: this may result in an over inflation of the risk factor. In order to mitigate the potential for this event the decision was made to implement a natural logarithm in order to normalise the data into a more collective state. This in turn will help to generate a risk factor that has less chance of procuring a large anomalous value, hence breaking the formula.

An alternative measurement could be formulated from the log file data. This would be done using the time between each successive instance of the same IP address. If the time between each successive instance of the same IP traffic is very small, these types of attack are better mitigated in real time. Cloudflare states that 'ability to implement page rules and populate those changes across the entire network is a critical feature in keeping a site online during an attack'. There is a possible situation that could occur, where web traffic from an IP may come at a staggered rate. However, this may also have the same affect, and the same potential to be malicious. 

After care consideration it was decided that it would be better to test more attacks by looking at the average time between IP instances. This was generated using the following formula: \[\lnb{\dfrac{Occurrences of IP }{43800 \times No. of log files}}\] It must be noted that the minimum value of this is 0.1 in order to prevent errors in the calculation. 

It would be appropriate to include an explanation of some values within this formula. The figure 43800 is the average number of minutes in a one month, based upon 30.4 days being the average number of days within a month. The number of log files is a dynamic value based on the number of files that have been uploaded to the software. This allows for multiple months worth of data to be analysed in one sitting.

\subsection{Response}


The response of the server is a good indicator of the legitimacy of a request, the server assigns a http code to the response. The decision was made to break down each request/response and apply an applicable risk level for each response code. The justifications for the applicable risk factors shall be discussed within this subsection. It should be noted that none of the literature reviews used this as an indicator for attack identification. As this is a path finding approach, the application of a risk factor for this element may be more likely to be adjusted through critical review after testing. (\cite{ErrorCodes})

A \textbf{400} error implies that the IP traffic instance made a BAD REQUEST. This would suggest that the user did not come to the site on a natural path from browsing. It would suggest that an error in logic was established during the request. On some webservers a 400 error message can be broken down into more detail to pinpoint the architectural issue within the request. It should be noted however that the webservers used for testing with this software do not use  IIS 7.0, IIS 7.5, or IIS 8.0 and hence will not break down full details of these 400 errors.

An invalid timeout response could be indicative of a situation where the GET request packet is invalid or has poor flagging standards. This could be a sign of a low rate bandwidth attack and hence it would be appropriate to apply a risk factor association for the greater 400 response error. 

An invalid destination header might suggest an incorrect http pathing request was input and may be indicative of a BOT attempting to formulate potential page targeting unsuccessfully. It may also indicate a handcrafted packet that has been constructed incorrectly or synthetically. As discussed in the literature review signs of a synthetic packet are highly indicative of a malicious request.

Due to the varied vectors that can produce a 400 request and the commonality of this error, it would be fair to apply a risk factor that does not overly penalise the event. After consideration a lower weight was applied to this error response with the ideology that it may stack after repeated instances of the same error.

A \textbf{401} error would indicate that the user is not authenticated to view the web-page. This could be due to an error in the server settings, which may lead to further unauthorised probes to evaluate weaknesses in the server configuration. This could also be an error raised from the occurrence of too many consecutive requests from the same IP or that the maximum number of allowed requests has been met. This would indicate that perhaps a server owner has set a limit on the amount of IP requests from the same source. There may, however, be legitimate reasons for a large number of requests from the same IP within a set time frame. A 401 may also occur if the website runs and API service, the API may generate a 401 error due to invalid credentials. These error messages should be given a suitably high risk factor rating as they are attempts to access forbidden pages or instances of denial. 

A \textbf{403} error is similar to a 401 in that a visitor has passed the authentication stage for reaching a page, however, for some reason they have failed to be permitted access to the page. This could be due to a read, write or execute violation. It could be an error generated from the denial of an IP even if an IP has been rejected due to historical data filtering, this would still generate a 403 error message. Therefore, this would suggest that this specific instance of traffic is most probably an attack, and also most probably automated. This would be fortified by the conclusion that, if a genuine visitor received a 403 message, they would eventually stop sending requests. Alternatively, an automated bot attacker would send a string of requests and continue to receive 403 messages. After consideration it was decided to give this error message a fairly high risk related flag.


A \textbf{404} error is an indicator that the page requested has not been found and is perhaps the most common error response encountered during response protocol. This potentially could mean that a visitor may be searching for a page that does not exist. Although this could be accidental or due to a change in directory navigation. If this error is repeated then this could be a sign of malicious activity. Due to the potential of an accidental or genuine 404 error, this was given a nominal risk factor for association.

A \textbf{429} error flags a scenario where a visitor has made too many requests within a set period of time. This could be indicative of a flood attack or a Denial of service attack. These appear to have a distinct overlapping with some of the criteria for a 401 error and therefore a similar weighting methodology should be applied. 


A \textbf{500} error tends to indicate where the IP is requesting data from. This could be an internal error such as the server is too busy. This may in fact be an indication that the server is under attack from a DoS attack, but does not suggest that this particular IP instance is the culprit. This could also be due to a rewriting or data storage error on the target servers. It could be argued that this is a negligible risk factor, however, a repeating IP receiving this response could accumulate a fairly high risk score. This visiting IP would begin to appear as the main culprit of a DNS flood attack through multiplication of this smaller risk factor.

A \textbf{200} response suggests that the visitor request has been successful. This is a potential indicator that a request is legitimate, however in the case of some discreetly synthesised attack packets, this is not always the case. For this reason a small risk reduction should be applied in order to mitigate risk factors from other variables. The response risk is a signed integer therefore when you make a large number of requests that return a 200 status the risk factor will be significantly reduced.



\subsection{Pages accessed}

The pages accessed from each visitor could be indicative of a malicious IP traffic status, hence some elements of risk should be appropriately applied. The following page requests that IP instances attempt to access and the associated risk weightings shall be discussed within this section.

If an IP is attempting to request access to WP-admin, which is the standard administrative area of Wordpress websites, it would be appropriate to apply a moderate risk rating. It is common for attackers to attempt brute force attacks at the "WP-admin". This is typically an attempt to unlock associated admin passwords by flood attempting a guess system in order to bypass security (\cite{Brute}). The attacker, which is normally a bot, will try as many username and password combinations as possible until they find the right one. This makes accounts with weaker password combinations particularly vulnerable, for example 'password123'. It should be noted that there may be genuine reasons that users may need to log into WP-admin. For example, a blogging website may have multiple bloggers, hence the reason numerous individuals would require access to the WP-admin page.

If an IP searches for a page containing the word 'login', it would be appropriate to apply a nominal risk factor. Although this could be a genuine request, the majority of authentic visitors would tend to navigate to the page by a procedural method. It should be noted that if there is no page containing the word login, they would get a higher risk rating cumulatively. This is due to receiving subsequent 404 errors, hence, distinguishing a malicious trend in the activity of the IP. The inverse is true if there is a login page, then a 200 response status would be applied to the communique reducing the risk factor slightly.

Although the formula has variables that appear at first to be separate and independent, by combining the risk related data enables the building of a 'larger picture' of the trends associated with a visiting IP.  An example of this would be the status and the requested URL. Through correlating variables malicious access attempts are highlighted and less aggressive traffic is mitigated accordingly.

The response size should be taken into consideration as an element of potential malignancy. If a request generates a server response with a size of zero, this may signify that the connection was closed before a response was returned. An event such as this could be indicative of a bot attack, or perhaps a GET request containing inappropriate flags.

\subsection{Country}

After consideration of data made available online, it appears that the country of origin for IP traffic can potentially infer a greater risk. After intensive research during the writing of this paper it was discovered that the USA was the origin of 45\% of all malicious traffic (\cite{Webattacks}). It should also be noted that the USA has the third highest population of active internet users in the world. Interestingly, this factor does not diminish the phenomenally high percentage of malicious traffic coming from that country. It therefore would be negligent to dismiss a country risk factor from the origin of IPs. It should be noted that some countries, such as China, have a particularly prohibitive fire-walling for internet access which is enforced through governmental regulations (\cite{China}). This may force malicious traffic from countries such as China through a VPN or proxy, in which case the initial country would be disguised from our software. It is important to note that although the software has the ability to detect the country of origin for the IP source location, it does not have the ability to assess the target websites location. This illustrates how important 'context' is, as for example, a letting agency in the UK would expect a higher rate of UK IP traffic. This highlights the importance of a human decision maker within the detection methodology in order to access the risk related factor in terms of the context of the event (\cite{cranor2008framework}). Due to the increasing ability for users to mask their location online it may be unfair to allocate a particularly high risk factor for this element. Instead it was more appropriate to apply minimal risk factor in order to assess its effects upon the monitoring of potential threats.

\subsection{Conclusion}

After consideration of the numerous risk associated variables the following risk weightings are proposed and have been documented in appendix \ref{Weights}. As has been shown in section \ref{Define Risk} there are inherent difficulties in defining risk using a mathematical approach. However, by utilising the information documented in the section above it is imagined that the following mathematical methodology can be used to define the risk of web traffic instances.

\[risk = (orrcancesOfipLog \times 0.6) + ((requestRisk+responseRisk) \times 0.3) + (countryRisk \times  0.1) \]

After data is run through the formula the outcome shall be a value between 1 and 100, The final integer is the value shown to the user in order to indicate overall risk. It is important to correctly weigh each variable with the appropriate \%. After careful analysis of the threat vectors it was decided to give the largest weights to the occurrences of IP. The decision was made to do this as, the vast majority of attacks share a correlated feature of large numbers of successive visits to the same site from the same IP. 

The request and response is the second most important factor in the formula; this is the accumulation of the response codes and the request is the page they are attempting to access. These two factors are combined equally to make up the greater 30\% of the overall formula. The decision to apply this weight was due to the multitude of different associated code risk factors. An accumulation of events could have too much impact upon the overall outcome of the formula, the variables are calculated independently as in future these may require separate weights.

As discussed during this chapter the country of origin is a particularly difficult weight to define, it has not been used as a key indicator of risk factor in prior papers it is a relatively experimental risk flag. It must also be noted that due to the increasing use of VPN activity online, it is becoming more and more unlikely to know for certain the true country of origin for any IP source. Hence the risk factor is at this experimental stage, particularly ineffectual on the outcome of the overall formula.

