\section{hypothesis}

%The hypothesis for this piece of was coming up with a better way of detecting malicious traffic. In essence, is there a way to identify and flag malicious IP requests with a formula based around the GET request characteristics. Many of the papers read as part of the literature review used an element of artificial intelligence in order to identify malicious traffic. In most cases these methods admitted to a degree of uncertainty in their methods of identification. For this reason it has been decided to implement some kinds of manual assessment of the incoming data in order to be overseen by a human moderator. It could be noted that many of the papers suggested that looking at a large data set may be problematic due to the constraints of processor depletion. This was high-lit during the literature review of \cite{Adi2015} and \cite{Adi2016}. When implementing a fix for this at the design level it was decided to use the access logs which are already collected by default on webservers. This should mean that no additional data is needed, and should cut out the factors raised in prior research regarding CPU redundancy. It is hypothesised that there is sufficient data within the log files in order to diagnosticate malicious traffic by analysis of the data. All the other tools that were implemented in the research uncovered during the literature review did not have any mention of user friendliness or any user interface design. Therefor, it could be argued that there is a definitive need for a user friendly UI, and this should be a critical element in the design architecture. It would be appropriate to see if the information presented to the user is easily interpreted. In order to assess this UI it would be beneficial to apply some kind of user testing in order to evaluate if the data is inherently too complex for individuals with minimal IT literacy to understand.

It is hypothesised that the process of collecting log files will be an efficient way of collecting data in terms of space and computational resource. It was high-lit in the \citeyear{staniford2002practical} paper by \citeauthor{staniford2002practical} that it is considered completely infeasible to save all network traffic for any sustained amount of time as there would be too much data to store. Hence collecting information with variables suitable for study in the condensed format would be invaluable for the performance of the software.

It is predicted that there is sufficient data within the log files in order to identify with a degree of certainty, potential malicious IP traffic patterns by looking at the characteristics of each instance.
It is hypothesised that a formula could be developed as an aide for the identification of potential malicious IP traffic instances by using the characteristics of each traffic instance as variables. It is proposed that the overview of a human moderator could look upon the potential threats and assess their elements in order to evaluate and take appropriate measures to confirm the status of a malicious IP instance and take any appropriate measures. 

It would be logical to postulate that a simple user interface would be required in order to ensure that website owners can analyse data in a understandable way. All the other tools that were implemented in the research uncovered during the literature review did not have any mention of user friendliness or any user interface design. Therefore, it could be argued that there is a definitive need for a user friendly UI, and this should be a critical element in the design architecture. Details and approaches for the intended user interface will be discussed later in the Chapter.