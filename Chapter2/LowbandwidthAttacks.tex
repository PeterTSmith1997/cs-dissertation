%!TeX root=Dissertation.tex

\section{Low-bandwidth attacks} \label{attack1}
%\begin{itemize}
 %   \item \textbf{What is a Low-bandwidth attack?}
  %  \item \textbf{Why are they hard to detect? }
   % \item Detection methods
    %\item How this will feed into my product
%\end{itemize}

A low-rate and low bandwidth attack is a different kind of attack in comparison to a normal DoS attack. A DoS attack works on overwhelming the server with requests, therefore it is easier to detect. For example, if you receive a large amount of traffic from a single IP; this is more than likely a DoS attack (REF NEEDED). A normal internet request operates based on an HTTP GET request to the web server that allows access to the site. The outcome of a DoS attack is the creation of a disruption between the clients and the web host. High-bandwidth attacks will keep trying to request multiple web pages at the same time to overwhelm the server however, a low-bandwidth attack sends a partial request and then may wait 20-30 seconds for example, it then sends new data, just enough to keep the connection open. One type of a Low-bandwidth attack is a slow loris attack; this is a layer-7 application attack, that only requires a small level of band-width and thus means the attacker can have continual use over their system and carry out normal activity. A web server will have a set number of sockets that it can have open at any one time, for this explanation we will choose to use 10 sockets. The aim of a slow loris is to open all 10 sockets and keep them busy, therefore, no new sockets can be opened and thus, no client interaction will take place. The difficulty with detecting these types of attacks is that the legitimate user may just have a slow internet connection, thus, making distinguishing these attacks from slow users difficult. 

There have been a couple of attempts to detect low-rate bandwidth attacks, some of these methods have been more successful than others.  The majority of websites are now using the HTTP/2 which uses multiple concurrent streams, the work done by \citeauthor{lowRateh2} in \citeyear{lowRateh2}, however, illustrated the vulnerability with this approach. The key thing found in their work was to monitor server workloads and CPU task handling. It must be noted that this was observed in a controlled and 'sterile' research environment. Host (attacker) and Target (victim) computer were set up with the single purpose of handling a controlled process of handling a website and monitoring a DoS attack. In reality this would not necessarily be indicative of a server network or typical of all round CPU usage or handling. For example, a computer hosting a website may also be running other background programs consuming CPU at varied levels, or, perhaps a website may also include an email service. 

It must be noted that the 2016 study carried out by Erwin Adi used Flashcrowd. They noted that there was at the time no reported study to ascertain whether or not attack traffic could or could not be concealed in a stealthy manor to appear as Flash Crowd Traffic. This may have potentiality made their methodology of simulating a DoS attack questionable.

\cite{tripathi2018slow} - Observed that the studies by Erwin Adi etc showed that the program Flash Crowd could be used to simulate a Dos Attack (Low Rate). They discovered that HTTP/2 is much more vulnerable to a wider variety of vector attacks than HTTP/1.1. It was observed that Chi squared test applied to detect network anomalies worked well as an approach via detecting the X2 value between the expected and observed traffic pattern.

Erwin Adi's 2015 paper assessed the vulnerability of Denial of service attacks on HTTP/2 using HTTP1.1 as a comparable predecessor.  It mainly focused on 'how' DoS attacks could be launched against HTTP/2 Services and the effects of Low-rate attacks on HTTP/2. It must be noted from the results that the team concluded HTTP/2 is more vulnerable to DoS attacks than its predecessor HTTP/2.

Erwin Adi's 2016 paper went on to test Flascrowd as simulator of a DoS attack, and the results the claim proved the effect of such attacks tested under four varied attack scenarios. Both the 2015 and 2016 studies used CPU and memory depletion as a measuring factor to determine the effects of each analysed attack.

My early criticism of all of the papers is that the methodology of their detection looked at a single server hosting one website. This is all very well, however in reality this practice would not be maintained. A server would host multiple websites and hence their method of detection does not propose a technique to isolate and identify DoS attacks targeting a particular website,.

Look at
\begin{itemize}
    \item \cite{tripathi2016secure}
    \item \cite{aiello2014line}
    \item \cite{tripathi2018slow}
\end{itemize}